{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/new-de-zoomcamp-449719-9c27773d9a31.json', 'r') as f:\n",
    "    os.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 05:16:47,480 - INFO - Downloading yellow_tripdata_2024-01.parquet\n",
      "2025-02-18 05:16:48,536 - INFO - Successfully processed yellow_tripdata_2024-01.parquet, found 2964624 records\n",
      "2025-02-18 05:17:14,269 - INFO - Downloading yellow_tripdata_2024-02.parquet\n",
      "2025-02-18 05:17:17,306 - INFO - Successfully processed yellow_tripdata_2024-02.parquet, found 3007526 records\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import dlt\n",
    "from dlt.sources import DltResource\n",
    "from typing import List, Dict, Iterator, Any, Tuple, Optional\n",
    "\n",
    "@dlt.source\n",
    "def nyc_taxi_data(\n",
    "    year: int = datetime.now().year,\n",
    "    start_month: int = 1,\n",
    "    end_month: int = 7,\n",
    "    max_workers: int = 1,\n",
    "    base_url: str = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    ") -> List[DltResource]:\n",
    "    \"\"\"\n",
    "    A dlt source for NYC Taxi trip data.\n",
    "\n",
    "    Args:\n",
    "        year: Year of the data to download (default: current year)\n",
    "        start_month: Starting month (1-12, default: 1)\n",
    "        end_month: Ending month (1-12, default: 7)\n",
    "        max_workers: Maximum number of parallel workers (default: 4)\n",
    "        base_url: Base URL for NYC taxi data (default: \"https://d37ci6vzurychx.cloudfront.net/trip-data\")\n",
    "\n",
    "    Returns:\n",
    "        List of dlt resources for NYC taxi data\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    @dlt.resource(write_disposition=\"merge\", primary_key=[\"vendorID\", \"tpep_pickup_datetime\"])\n",
    "    def yellow_taxi_trips() -> Iterator[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Resource that yields NYC yellow taxi trip data for a specified time range.\n",
    "        Data is downloaded and yielded in chunks for memory efficiency.\n",
    "        \"\"\"\n",
    "        def download_month_data(year: int, month: int) -> Optional[List[Dict[str, Any]]]:\n",
    "            file_name = f\"yellow_tripdata_{year}-{month:02d}.parquet\"\n",
    "            url = f\"{base_url}/{file_name}\"\n",
    "            \n",
    "            try:\n",
    "                logger.info(f\"Downloading {file_name}\")\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Save to a temporary file\n",
    "                temp_path = f\"/tmp/{file_name}\"\n",
    "                with open(temp_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1000):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                # Read parquet file into a pandas DataFrame and convert to dict\n",
    "                import pandas as pd\n",
    "                df = pd.read_parquet(temp_path)\n",
    "                \n",
    "                # Clean up temporary file\n",
    "                os.remove(temp_path)\n",
    "                \n",
    "                # Add metadata columns for tracking\n",
    "                df['source_file'] = file_name\n",
    "                df['ingestion_timestamp'] = datetime.now().isoformat()\n",
    "                \n",
    "                logger.info(f\"Successfully processed {file_name}, found {len(df)} records\")\n",
    "                return df.to_dict('records')\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error downloading {file_name}: {str(e)}\")\n",
    "                return None\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(download_month_data, year, month): (year, month)\n",
    "                for month in range(start_month, end_month + 1)\n",
    "            }\n",
    "            \n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    for record in result:\n",
    "                        yield record\n",
    "    \n",
    "    return [yellow_taxi_trips]\n",
    "\n",
    "def run_nyc_taxi_pipeline():\n",
    "    \"\"\"\n",
    "    Main function to run the NYC taxi pipeline to BigQuery\n",
    "    \"\"\"\n",
    "    # Configure the pipeline to load data to BigQuery\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name='nyc_taxi_data',\n",
    "        destination='bigquery',\n",
    "        dataset_name='nyc_taxi',\n",
    "        dev_mode=True\n",
    "    )\n",
    "    \n",
    "    # Load the data\n",
    "    load_info = pipeline.run(\n",
    "        nyc_taxi_data(\n",
    "            year=2024,  # can be parameterized\n",
    "            start_month=1,  # can be parameterized\n",
    "            end_month=2,   # can be parameterized\n",
    "            max_workers=1  # can be parameterized\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Load info: {load_info}\")\n",
    "    \n",
    "    # Return info for verification\n",
    "    return load_info\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_nyc_taxi_pipeline()\n",
    "\n",
    "# To customize pipeline for BigQuery, you can use the following:\n",
    "\"\"\"\n",
    "import dlt\n",
    "\n",
    "# Create credentials from service account key file\n",
    "credentials = dlt.secrets.service_account_key_file('/path/to/credentials.json')\n",
    "\n",
    "# Initialize pipeline with BigQuery-specific configuration\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='nyc_taxi_data',\n",
    "    destination='bigquery',\n",
    "    dataset_name='nyc_taxi',\n",
    "    credentials=credentials,\n",
    "    full_refresh=False, # set to True to replace all data\n",
    "    progress=True\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline.run(nyc_taxi_data())\n",
    "\"\"\"\n",
    "\n",
    "# You can also run from command line with:\n",
    "# python nyc_taxi_dlt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dlt\n",
    "from dlt.sources.credentials import GcpServiceAccountCredentials\n",
    "from dlt.destinations import bigquery\n",
    "\n",
    "# Retrieve credentials from the environment variable\n",
    "creds_dict = os.getenv('BIGQUERY_CREDENTIALS')\n",
    "\n",
    "# Create credentials instance and parse them from a native representation\n",
    "gcp_credentials = GcpServiceAccountCredentials()\n",
    "gcp_credentials.parse_native_representation(creds_dict)\n",
    "\n",
    "# Pass the credentials to the BigQuery destination\n",
    "# pipeline = dlt.pipeline(destination=bigquery(credentials=gcp_credentials))\n",
    "# pipeline.run([{\"key1\": \"value1\"}], table_name=\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "###############################\n",
    "# NYCTaxiDataLoader Definition\n",
    "###############################\n",
    "class NYCTaxiDataLoader:\n",
    "    def __init__(self, gcp_credentials_path, bucket_name, file_format, base_url=\"https://d37ci6vzurychx.cloudfront.net/trip-data\"):\n",
    "        \"\"\"\n",
    "        Initialize the NYC Taxi Data Loader.\n",
    "        \n",
    "        Args:\n",
    "            gcp_credentials_path (str): Path to GCP service account JSON file.\n",
    "            bucket_name (str): Name of the GCS bucket.\n",
    "            file_format (str): File format to download ('parquet', 'csv', or 'excel').\n",
    "            base_url (str): Base URL for NYC taxi data.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.bucket_name = bucket_name\n",
    "        self.file_format = file_format.lower()\n",
    "        self.credentials = service_account.Credentials.from_service_account_file(gcp_credentials_path)\n",
    "        self.storage_client = storage.Client(credentials=self.credentials)\n",
    "        self.bucket = self.storage_client.bucket(bucket_name)\n",
    "        \n",
    "        # Set up logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def _generate_file_url(self, year, month):\n",
    "        \"\"\"\n",
    "        Generate the file URL for a given month/year.\n",
    "        For Excel files, the extension is assumed to be 'xlsx'.\n",
    "        \"\"\"\n",
    "        ext = self.file_format\n",
    "        if ext == \"excel\":\n",
    "            ext = \"xlsx\"\n",
    "        file_name = f\"yellow_tripdata_{year}-{month:02d}.{ext}\"\n",
    "        return f\"{self.base_url}/{file_name}\", file_name\n",
    "\n",
    "    def download_file(self, year, month):\n",
    "        \"\"\"\n",
    "        Download a single month's taxi data.\n",
    "        \"\"\"\n",
    "        url, file_name = self._generate_file_url(year, month)\n",
    "        local_path = f\"/tmp/{file_name}\"\n",
    "        try:\n",
    "            self.logger.info(f\"Downloading {file_name} from {url}\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            return local_path, file_name\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading {file_name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def upload_to_gcs(self, local_path, blob_name):\n",
    "        \"\"\"\n",
    "        Upload the local file to GCS.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            blob = self.bucket.blob(blob_name)\n",
    "            self.logger.info(f\"Uploading {blob_name} to GCS bucket {self.bucket_name}\")\n",
    "            blob.upload_from_filename(local_path)\n",
    "            os.remove(local_path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error uploading {blob_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def process_month(self, year, month):\n",
    "        \"\"\"\n",
    "        Process a single month (download then upload).\n",
    "        \"\"\"\n",
    "        result = self.download_file(year, month)\n",
    "        if result:\n",
    "            local_path, blob_name = result\n",
    "            return self.upload_to_gcs(local_path, blob_name)\n",
    "        return False\n",
    "\n",
    "    def process_months(self, year, start_month=1, end_month=7, max_workers=4):\n",
    "        \"\"\"\n",
    "        Process multiple months in parallel.\n",
    "        \"\"\"\n",
    "        successful_months = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_month = {\n",
    "                executor.submit(self.process_month, year, month): month\n",
    "                for month in range(start_month, end_month + 1)\n",
    "            }\n",
    "            for future in future_to_month:\n",
    "                month = future_to_month[future]\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        successful_months.append(month)\n",
    "                        self.logger.info(f\"Successfully processed month {month}\")\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Failed to process month {month}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing month {month}: {str(e)}\")\n",
    "        return successful_months\n",
    "\n",
    "#################################\n",
    "# Command-Line Argument Parsing\n",
    "#################################\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"NYCTaxiDataLoader + DLT Pipeline: Download NYC taxi data (in a single specified format) to GCS or run a DLT pipeline to load it to BigQuery.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--credentials',\n",
    "        required=True,\n",
    "        help='Path to GCP service account credentials JSON file'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--bucket',\n",
    "        required=True,\n",
    "        help='Name of the GCS bucket'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--year',\n",
    "        type=int,\n",
    "        default=datetime.now().year,\n",
    "        help='Year of data to download (default: current year)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--start-month',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        choices=range(1, 13),\n",
    "        help='Starting month (1-12)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--end-month',\n",
    "        type=int,\n",
    "        default=7,\n",
    "        choices=range(1, 13),\n",
    "        help='Ending month (1-12)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--workers',\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help='Number of parallel workers (default: 4)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--file-format',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        choices=['parquet', 'csv', 'excel'],\n",
    "        help='File format to process: parquet, csv, or excel'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--action',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        choices=['download', 'pipeline'],\n",
    "        help='Action to perform: \"download\" to load files into GCS, or \"pipeline\" to run the DLT pipeline to load from GCS to BigQuery'\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "#################################\n",
    "# DLT Pipeline Code\n",
    "#################################\n",
    "# Import DLT libraries and filesystem utilities\n",
    "import dlt\n",
    "from dlt.common.storages.fsspec_filesystem import FileItemDict\n",
    "from dlt.common.typing import TDataItems\n",
    "from dlt.sources.filesystem import filesystem, read_parquet, read_csv\n",
    "from typing import Iterator\n",
    "\n",
    "def get_bucket_url(bucket_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Construct the filesystem URL for GCS.\n",
    "    (For GCS, the URL scheme is typically \"gs://\")\n",
    "    \"\"\"\n",
    "    return f\"gs://{bucket_name}\"\n",
    "\n",
    "# Custom transformer to read Excel files using yield.\n",
    "@dlt.transformer(standalone=True)\n",
    "def read_excel(items: Iterator[FileItemDict], sheet_name: str) -> Iterator[TDataItems]:\n",
    "    import pandas as pd\n",
    "    for file_obj in items:\n",
    "        with file_obj.open() as f:\n",
    "            df = pd.read_excel(f, sheet_name=sheet_name)\n",
    "            for record in df.to_dict(orient=\"records\"):\n",
    "                yield record\n",
    "\n",
    "def run_dlt_pipeline(bucket_name: str, file_format: str, sheet_name: str = \"Sheet1\"):\n",
    "    \"\"\"\n",
    "    Run the DLT pipeline that reads files from the GCS bucket and loads them to BigQuery.\n",
    "    Only files matching the specified file format are processed.\n",
    "    \"\"\"\n",
    "    BUCKET_URL = get_bucket_url(bucket_name)\n",
    "    # Choose the appropriate file resource based on the file_format.\n",
    "    if file_format.lower() == \"csv\":\n",
    "        resource = filesystem(\n",
    "            bucket_url=BUCKET_URL,\n",
    "            file_glob=\"**/*.csv\"\n",
    "        ) | read_csv()\n",
    "    elif file_format.lower() == \"parquet\":\n",
    "        resource = filesystem(\n",
    "            bucket_url=BUCKET_URL,\n",
    "            file_glob=\"**/*.parquet\"\n",
    "        ) | read_parquet()\n",
    "    elif file_format.lower() == \"excel\":\n",
    "        resource = filesystem(\n",
    "            bucket_url=BUCKET_URL,\n",
    "            file_glob=\"**/*.xlsx\"\n",
    "        ) | read_excel(sheet_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "\n",
    "    # Define the DLT pipeline. In this example, we load into BigQuery.\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"nyc_taxi_pipeline\",\n",
    "        destination=bigquery(credentials=gcp_credentials),\n",
    "        dataset_name=\"nyc_taxi_data\"\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline loading the data into a table called \"nyc_taxi_table\".\n",
    "    load_info = pipeline.run(resource.with_name(\"nyc_taxi_table\"))\n",
    "    print(load_info)\n",
    "\n",
    "###############################\n",
    "# Main Function\n",
    "###############################\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # The user-specified file format controls both the download/upload and the DLT processing.\n",
    "    if args.action == \"download\":\n",
    "        loader = NYCTaxiDataLoader(args.credentials, args.bucket, args.file_format)\n",
    "        successful_months = loader.process_months(\n",
    "            year=args.year,\n",
    "            start_month=args.start_month,\n",
    "            end_month=args.end_month,\n",
    "            max_workers=args.workers\n",
    "        )\n",
    "        total_months = args.end_month - args.start_month + 1\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Successfully processed months: {successful_months}\")\n",
    "        print(f\"Total successful: {len(successful_months)}\")\n",
    "        print(f\"Total failed: {total_months - len(successful_months)}\")\n",
    "    elif args.action == \"pipeline\":\n",
    "        # For Excel files, you can adjust the sheet name as needed.\n",
    "        run_dlt_pipeline(args.bucket, args.file_format, sheet_name=\"Sheet1\")\n",
    "    else:\n",
    "        print(\"Invalid action specified. Use 'download' or 'pipeline'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python combined_script.py --credentials path/to/creds.json --bucket your-gcs-bucket --year 2021 --start-month 1 --end-month 3 --file-format csv --action download\n",
    "\n",
    "# python combined_script.py --credentials path/to/creds.json --bucket your-gcs-bucket --file-format csv --action pipeline\n",
    "\n",
    "#or use os.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = to set dlt bigquery env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
