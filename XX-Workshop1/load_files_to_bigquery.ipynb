{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/new-de-zoomcamp-449719-9c27773d9a31.json', 'r') as f:\n",
    "    os.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 05:16:47,480 - INFO - Downloading yellow_tripdata_2024-01.parquet\n",
      "2025-02-18 05:16:48,536 - INFO - Successfully processed yellow_tripdata_2024-01.parquet, found 2964624 records\n",
      "2025-02-18 05:17:14,269 - INFO - Downloading yellow_tripdata_2024-02.parquet\n",
      "2025-02-18 05:17:17,306 - INFO - Successfully processed yellow_tripdata_2024-02.parquet, found 3007526 records\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import dlt\n",
    "from dlt.sources import DltResource\n",
    "from typing import List, Dict, Iterator, Any, Tuple, Optional\n",
    "\n",
    "@dlt.source\n",
    "def nyc_taxi_data(\n",
    "    year: int = datetime.now().year,\n",
    "    start_month: int = 1,\n",
    "    end_month: int = 7,\n",
    "    max_workers: int = 1,\n",
    "    base_url: str = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    ") -> List[DltResource]:\n",
    "    \"\"\"\n",
    "    A dlt source for NYC Taxi trip data.\n",
    "\n",
    "    Args:\n",
    "        year: Year of the data to download (default: current year)\n",
    "        start_month: Starting month (1-12, default: 1)\n",
    "        end_month: Ending month (1-12, default: 7)\n",
    "        max_workers: Maximum number of parallel workers (default: 4)\n",
    "        base_url: Base URL for NYC taxi data (default: \"https://d37ci6vzurychx.cloudfront.net/trip-data\")\n",
    "\n",
    "    Returns:\n",
    "        List of dlt resources for NYC taxi data\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    @dlt.resource(write_disposition=\"merge\", primary_key=[\"vendorID\", \"tpep_pickup_datetime\"])\n",
    "    def yellow_taxi_trips() -> Iterator[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Resource that yields NYC yellow taxi trip data for a specified time range.\n",
    "        Data is downloaded and yielded in chunks for memory efficiency.\n",
    "        \"\"\"\n",
    "        def download_month_data(year: int, month: int) -> Optional[List[Dict[str, Any]]]:\n",
    "            file_name = f\"yellow_tripdata_{year}-{month:02d}.parquet\"\n",
    "            url = f\"{base_url}/{file_name}\"\n",
    "            \n",
    "            try:\n",
    "                logger.info(f\"Downloading {file_name}\")\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Save to a temporary file\n",
    "                temp_path = f\"/tmp/{file_name}\"\n",
    "                with open(temp_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1000):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                # Read parquet file into a pandas DataFrame and convert to dict\n",
    "                import pandas as pd\n",
    "                df = pd.read_parquet(temp_path)\n",
    "                \n",
    "                # Clean up temporary file\n",
    "                os.remove(temp_path)\n",
    "                \n",
    "                # Add metadata columns for tracking\n",
    "                df['source_file'] = file_name\n",
    "                df['ingestion_timestamp'] = datetime.now().isoformat()\n",
    "                \n",
    "                logger.info(f\"Successfully processed {file_name}, found {len(df)} records\")\n",
    "                return df.to_dict('records')\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error downloading {file_name}: {str(e)}\")\n",
    "                return None\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(download_month_data, year, month): (year, month)\n",
    "                for month in range(start_month, end_month + 1)\n",
    "            }\n",
    "            \n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    for record in result:\n",
    "                        yield record\n",
    "    \n",
    "    return [yellow_taxi_trips]\n",
    "\n",
    "def run_nyc_taxi_pipeline():\n",
    "    \"\"\"\n",
    "    Main function to run the NYC taxi pipeline to BigQuery\n",
    "    \"\"\"\n",
    "    # Configure the pipeline to load data to BigQuery\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name='nyc_taxi_data',\n",
    "        destination='bigquery',\n",
    "        dataset_name='nyc_taxi',\n",
    "        dev_mode=True\n",
    "    )\n",
    "    \n",
    "    # Load the data\n",
    "    load_info = pipeline.run(\n",
    "        nyc_taxi_data(\n",
    "            year=2024,  # can be parameterized\n",
    "            start_month=1,  # can be parameterized\n",
    "            end_month=2,   # can be parameterized\n",
    "            max_workers=1  # can be parameterized\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Load info: {load_info}\")\n",
    "    \n",
    "    # Return info for verification\n",
    "    return load_info\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_nyc_taxi_pipeline()\n",
    "\n",
    "# To customize pipeline for BigQuery, you can use the following:\n",
    "\"\"\"\n",
    "import dlt\n",
    "\n",
    "# Create credentials from service account key file\n",
    "credentials = dlt.secrets.service_account_key_file('/path/to/credentials.json')\n",
    "\n",
    "# Initialize pipeline with BigQuery-specific configuration\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='nyc_taxi_data',\n",
    "    destination='bigquery',\n",
    "    dataset_name='nyc_taxi',\n",
    "    credentials=credentials,\n",
    "    full_refresh=False, # set to True to replace all data\n",
    "    progress=True\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline.run(nyc_taxi_data())\n",
    "\"\"\"\n",
    "\n",
    "# You can also run from command line with:\n",
    "# python nyc_taxi_dlt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dlt\n",
    "from dlt.sources.credentials import GcpServiceAccountCredentials\n",
    "from dlt.destinations import bigquery\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_account_dict = {\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"...\",\n",
    "  \"private_key_id\": \"...\",\n",
    "  \"private_key\": \"...\",\n",
    "  \"client_email\": \"...\",\n",
    "  \"client_id\": \"...\",\n",
    "  \"auth_uri\": \"...\",\n",
    "  \"token_uri\": \"...\",\n",
    "}\n",
    "os.environ[\"BIGQUERY_CREDENTIALS\"] = json.dumps(service_account_dict)\n",
    "\n",
    "# Retrieve credentials from the environment variable\n",
    "creds_dict = os.getenv('BIGQUERY_CREDENTIALS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create credentials instance and parse them from a native representation\n",
    "gcp_credentials = GcpServiceAccountCredentials()\n",
    "gcp_credentials.parse_native_representation(creds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadInfo(pipeline=<dlt.pipeline.pipeline.Pipeline object at 0x791acdcc9730>, metrics={'1739941112.85621': [{'started_at': DateTime(2025, 2, 19, 4, 58, 33, 30568, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2025, 2, 19, 4, 58, 46, 316351, tzinfo=Timezone('UTC')), 'job_metrics': {'rides.7315fd4f93.jsonl': LoadJobMetrics(job_id='rides.7315fd4f93.jsonl', file_path='/home/codespace/.dlt/pipelines/taxi_data2/load/normalized/1739941112.85621/started_jobs/rides.7315fd4f93.0.jsonl', table_name='rides', started_at=DateTime(2025, 2, 19, 4, 58, 41, 44726, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 2, 19, 4, 58, 44, 199908, tzinfo=Timezone('UTC')), state='completed', remote_url=None), '_dlt_pipeline_state.6ad42ee803.jsonl': LoadJobMetrics(job_id='_dlt_pipeline_state.6ad42ee803.jsonl', file_path='/home/codespace/.dlt/pipelines/taxi_data2/load/normalized/1739941112.85621/started_jobs/_dlt_pipeline_state.6ad42ee803.0.jsonl', table_name='_dlt_pipeline_state', started_at=DateTime(2025, 2, 19, 4, 58, 41, 18125, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 2, 19, 4, 58, 44, 293305, tzinfo=Timezone('UTC')), state='completed', remote_url=None)}}]}, destination_type='dlt.destinations.bigquery', destination_displayable_credentials='dbt-service-account@de-zoomcamp-449719.iam.gserviceaccount.com@de-zoomcamp-449719', destination_name='bigquery', environment=None, staging_type=None, staging_name=None, staging_displayable_credentials=None, destination_fingerprint='Qyy18vzHlxeR5Z6o8IrW', dataset_name='taxi_rides', loads_ids=['1739941112.85621'], load_packages=[LoadPackageInfo(load_id='1739941112.85621', package_path='/home/codespace/.dlt/pipelines/taxi_data2/load/loaded/1739941112.85621', state='loaded', schema=Schema taxi_data2 at 133156291889488, schema_update={'_dlt_pipeline_state': {'columns': {'version': {'name': 'version', 'data_type': 'bigint', 'nullable': False}, 'engine_version': {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, 'pipeline_name': {'name': 'pipeline_name', 'data_type': 'text', 'nullable': False}, 'state': {'name': 'state', 'data_type': 'text', 'nullable': False}, 'created_at': {'name': 'created_at', 'data_type': 'timestamp', 'nullable': False}, 'version_hash': {'name': 'version_hash', 'data_type': 'text', 'nullable': True}, '_dlt_load_id': {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, '_dlt_id': {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True, 'row_key': True}}, 'write_disposition': 'append', 'file_format': 'preferred', 'name': '_dlt_pipeline_state', 'resource': '_dlt_pipeline_state', 'x-normalizer': {'seen-data': True}}, 'rides': {'name': 'rides', 'columns': {'key1': {'name': 'key1', 'data_type': 'text', 'nullable': True}, '_dlt_load_id': {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, '_dlt_id': {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True, 'row_key': True}}, 'write_disposition': 'replace', 'resource': 'rides', 'x-normalizer': {'seen-data': True}, 'x-bigquery-table-description': None, 'x-bigquery-autodetect-schema': False}, '_dlt_loads': {'name': '_dlt_loads', 'columns': {'load_id': {'name': 'load_id', 'data_type': 'text', 'nullable': False}, 'schema_name': {'name': 'schema_name', 'data_type': 'text', 'nullable': True}, 'status': {'name': 'status', 'data_type': 'bigint', 'nullable': False}, 'inserted_at': {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, 'schema_version_hash': {'name': 'schema_version_hash', 'data_type': 'text', 'nullable': True}}, 'write_disposition': 'skip', 'resource': '_dlt_loads', 'description': 'Created by DLT. Tracks completed loads'}, '_dlt_version': {'name': '_dlt_version', 'columns': {'version': {'name': 'version', 'data_type': 'bigint', 'nullable': False}, 'engine_version': {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, 'inserted_at': {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, 'schema_name': {'name': 'schema_name', 'data_type': 'text', 'nullable': False}, 'version_hash': {'name': 'version_hash', 'data_type': 'text', 'nullable': False}, 'schema': {'name': 'schema', 'data_type': 'text', 'nullable': False}}, 'write_disposition': 'skip', 'resource': '_dlt_version', 'description': 'Created by DLT. Tracks schema updates'}}, completed_at=DateTime(2025, 2, 19, 4, 58, 46, 306012, tzinfo=Timezone('UTC')), jobs={'completed_jobs': [LoadJobInfo(state='completed_jobs', file_path='/home/codespace/.dlt/pipelines/taxi_data2/load/loaded/1739941112.85621/completed_jobs/_dlt_pipeline_state.6ad42ee803.0.jsonl', file_size=478, created_at=DateTime(2025, 2, 19, 4, 58, 32, 978012, tzinfo=Timezone('UTC')), elapsed=13.327999830245972, job_file_info=ParsedLoadJobFileName(table_name='_dlt_pipeline_state', file_id='6ad42ee803', retry_count=0, file_format='jsonl'), failed_message=None), LoadJobInfo(state='completed_jobs', file_path='/home/codespace/.dlt/pipelines/taxi_data2/load/loaded/1739941112.85621/completed_jobs/rides.7315fd4f93.0.jsonl', file_size=119, created_at=DateTime(2025, 2, 19, 4, 58, 32, 974012, tzinfo=Timezone('UTC')), elapsed=13.331999778747559, job_file_info=ParsedLoadJobFileName(table_name='rides', file_id='7315fd4f93', retry_count=0, file_format='jsonl'), failed_message=None)], 'started_jobs': [], 'failed_jobs': [], 'new_jobs': []})], first_run=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the credentials to the BigQuery destination\n",
    "pipeline = dlt.pipeline(pipeline_name=\"taxi_data2\", destination=bigquery(credentials=gcp_credentials), dataset_name=\"taxi_rides\")\n",
    "pipeline.run([{\"key1\": \"value1\"}], table_name=\"rides\", write_disposition=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_path = \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/new-de-zoomcamp-449719-9c27773d9a31.json\"\n",
    "with open(creds_path, \"r\") as f:\n",
    "        creds2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"BIGQUERY_CREDENTIALS\"] = json.dumps(creds2)\n",
    "\n",
    "# Retrieve credentials from the environment variable\n",
    "creds_dict = os.getenv('BIGQUERY_CREDENTIALS')\n",
    "gcp_credentials2 = GcpServiceAccountCredentials()\n",
    "gcp_credentials2.parse_native_representation(creds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadInfo(pipeline=<dlt.pipeline.pipeline.Pipeline object at 0x791add7c8410>, metrics={'1739941279.3852673': [{'started_at': DateTime(2025, 2, 19, 5, 1, 19, 483979, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2025, 2, 19, 5, 1, 34, 18406, tzinfo=Timezone('UTC')), 'job_metrics': {'ridesnew.c690c8ca6a.jsonl': LoadJobMetrics(job_id='ridesnew.c690c8ca6a.jsonl', file_path='/home/codespace/.dlt/pipelines/taxi_rides2/load/normalized/1739941279.3852673/started_jobs/ridesnew.c690c8ca6a.0.jsonl', table_name='ridesnew', started_at=DateTime(2025, 2, 19, 5, 1, 28, 555161, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 2, 19, 5, 1, 31, 778529, tzinfo=Timezone('UTC')), state='completed', remote_url=None), '_dlt_pipeline_state.155280ff82.jsonl': LoadJobMetrics(job_id='_dlt_pipeline_state.155280ff82.jsonl', file_path='/home/codespace/.dlt/pipelines/taxi_rides2/load/normalized/1739941279.3852673/started_jobs/_dlt_pipeline_state.155280ff82.0.jsonl', table_name='_dlt_pipeline_state', started_at=DateTime(2025, 2, 19, 5, 1, 28, 528974, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 2, 19, 5, 1, 31, 833485, tzinfo=Timezone('UTC')), state='completed', remote_url=None)}}]}, destination_type='dlt.destinations.bigquery', destination_displayable_credentials='dbt-service-account@de-zoomcamp-449719.iam.gserviceaccount.com@de-zoomcamp-449719', destination_name='bigquery', environment=None, staging_type=None, staging_name=None, staging_displayable_credentials=None, destination_fingerprint='Qyy18vzHlxeR5Z6o8IrW', dataset_name='taxi_rides_new', loads_ids=['1739941279.3852673'], load_packages=[LoadPackageInfo(load_id='1739941279.3852673', package_path='/home/codespace/.dlt/pipelines/taxi_rides2/load/loaded/1739941279.3852673', state='loaded', schema=Schema taxi_rides2 at 133156290470416, schema_update={'_dlt_pipeline_state': {'columns': {'version': {'name': 'version', 'data_type': 'bigint', 'nullable': False}, 'engine_version': {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, 'pipeline_name': {'name': 'pipeline_name', 'data_type': 'text', 'nullable': False}, 'state': {'name': 'state', 'data_type': 'text', 'nullable': False}, 'created_at': {'name': 'created_at', 'data_type': 'timestamp', 'nullable': False}, 'version_hash': {'name': 'version_hash', 'data_type': 'text', 'nullable': True}, '_dlt_load_id': {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, '_dlt_id': {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True, 'row_key': True}}, 'write_disposition': 'append', 'file_format': 'preferred', 'name': '_dlt_pipeline_state', 'resource': '_dlt_pipeline_state', 'x-normalizer': {'seen-data': True}}, 'ridesnew': {'name': 'ridesnew', 'columns': {'key1': {'name': 'key1', 'data_type': 'text', 'nullable': True}, '_dlt_load_id': {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, '_dlt_id': {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True, 'row_key': True}}, 'write_disposition': 'replace', 'resource': 'ridesnew', 'x-normalizer': {'seen-data': True}, 'x-bigquery-table-description': None, 'x-bigquery-autodetect-schema': False}, '_dlt_loads': {'name': '_dlt_loads', 'columns': {'load_id': {'name': 'load_id', 'data_type': 'text', 'nullable': False}, 'schema_name': {'name': 'schema_name', 'data_type': 'text', 'nullable': True}, 'status': {'name': 'status', 'data_type': 'bigint', 'nullable': False}, 'inserted_at': {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, 'schema_version_hash': {'name': 'schema_version_hash', 'data_type': 'text', 'nullable': True}}, 'write_disposition': 'skip', 'resource': '_dlt_loads', 'description': 'Created by DLT. Tracks completed loads'}, '_dlt_version': {'name': '_dlt_version', 'columns': {'version': {'name': 'version', 'data_type': 'bigint', 'nullable': False}, 'engine_version': {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, 'inserted_at': {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, 'schema_name': {'name': 'schema_name', 'data_type': 'text', 'nullable': False}, 'version_hash': {'name': 'version_hash', 'data_type': 'text', 'nullable': False}, 'schema': {'name': 'schema', 'data_type': 'text', 'nullable': False}}, 'write_disposition': 'skip', 'resource': '_dlt_version', 'description': 'Created by DLT. Tracks schema updates'}}, completed_at=DateTime(2025, 2, 19, 5, 1, 34, 6006, tzinfo=Timezone('UTC')), jobs={'completed_jobs': [LoadJobInfo(state='completed_jobs', file_path='/home/codespace/.dlt/pipelines/taxi_rides2/load/loaded/1739941279.3852673/completed_jobs/_dlt_pipeline_state.155280ff82.0.jsonl', file_size=485, created_at=DateTime(2025, 2, 19, 5, 1, 19, 450006, tzinfo=Timezone('UTC')), elapsed=14.555999755859375, job_file_info=ParsedLoadJobFileName(table_name='_dlt_pipeline_state', file_id='155280ff82', retry_count=0, file_format='jsonl'), failed_message=None), LoadJobInfo(state='completed_jobs', file_path='/home/codespace/.dlt/pipelines/taxi_rides2/load/loaded/1739941279.3852673/completed_jobs/ridesnew.c690c8ca6a.0.jsonl', file_size=125, created_at=DateTime(2025, 2, 19, 5, 1, 19, 450006, tzinfo=Timezone('UTC')), elapsed=14.555999755859375, job_file_info=ParsedLoadJobFileName(table_name='ridesnew', file_id='c690c8ca6a', retry_count=0, file_format='jsonl'), failed_message=None)], 'started_jobs': [], 'failed_jobs': [], 'new_jobs': []})], first_run=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the credentials to the BigQuery destination, dataset name is the bigquery dataset and table name is the table name in the dataset.\n",
    "#if dataset name is not given, I think it will be generated from the pipeline_name\n",
    "pipeline = dlt.pipeline(pipeline_name=\"taxi_rides2\", destination=bigquery(credentials=gcp_credentials2), dataset_name=\"taxi_rides_new\")\n",
    "pipeline.run([{\"key1\": \"value1\"}], table_name=\"ridesnew\", write_disposition=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 04:54:16,242|[ERROR]|1668|133155587614464|dlt|load.py|w_run_job:247|Terminal exception in job temp.949a865c3a.jsonl in file /home/codespace/.dlt/pipelines/dlt_ipykernel_launcher/load/normalized/1739940854.5797727/started_jobs/temp.949a865c3a.0.jsonl\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/google/cloud/bigquery/client.py\", line 2599, in load_table_from_file\n",
      "    response = self._do_resumable_upload(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/google/cloud/bigquery/client.py\", line 3019, in _do_resumable_upload\n",
      "    upload, transport = self._initiate_resumable_upload(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/google/cloud/bigquery/client.py\", line 3088, in _initiate_resumable_upload\n",
      "    upload.initiate(\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/google/resumable_media/requests/upload.py\", line 420, in initiate\n",
      "    return _request_helpers.wait_and_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/google/resumable_media/requests/_request_helpers.py\", line 155, in wait_and_retry\n",
      "    response = func()\n",
      "               ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/google/resumable_media/requests/upload.py\", line 416, in retriable_request\n",
      "    self._process_initiate_response(result)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/google/resumable_media/_upload.py\", line 518, in _process_initiate_response\n",
      "    _helpers.require_status_code(\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/google/resumable_media/_helpers.py\", line 108, in require_status_code\n",
      "    raise common.InvalidResponse(\n",
      "google.resumable_media.common.InvalidResponse: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.CREATED: 201>)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/destinations/impl/bigquery/bigquery.py\", line 74, in run\n",
      "    self._bq_load_job = self._job_client._create_load_job(self._load_table, self._file_path)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/destinations/impl/bigquery/bigquery.py\", line 454, in _create_load_job\n",
      "    return self.sql_client.native_connection.load_table_from_file(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/google/cloud/bigquery/client.py\", line 2607, in load_table_from_file\n",
      "    raise exceptions.from_http_response(exc.response)\n",
      "google.api_core.exceptions.NotFound: 404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/de-zoomcamp-449719/jobs?uploadType=resumable: Not found: Table de-zoomcamp-449719:dlt_ipykernel_launcher_dataset.temp was not found in location US\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/common/destination/client.py\", line 395, in run_managed\n",
      "    self.run()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/destinations/impl/bigquery/bigquery.py\", line 80, in run\n",
      "    raise DatabaseUndefinedRelation(gace) from gace\n",
      "dlt.destinations.exceptions.DatabaseUndefinedRelation: 404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/de-zoomcamp-449719/jobs?uploadType=resumable: Not found: Table de-zoomcamp-449719:dlt_ipykernel_launcher_dataset.temp was not found in location US\n",
      "2025-02-19 04:54:16,244|[ERROR]|1668|133157126685312|dlt|load.py|complete_jobs:413|Job for temp.949a865c3a.jsonl failed terminally in load 1739940854.5797727 with message 404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/de-zoomcamp-449719/jobs?uploadType=resumable: Not found: Table de-zoomcamp-449719:dlt_ipykernel_launcher_dataset.temp was not found in location US\n"
     ]
    },
    {
     "ename": "PipelineStepFailed",
     "evalue": "Pipeline execution failed at stage load with exception:\n\n<class 'dlt.load.exceptions.LoadClientJobFailed'>\nJob for temp.949a865c3a.jsonl failed terminally in load 1739940854.5797727 with message 404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/de-zoomcamp-449719/jobs?uploadType=resumable: Not found: Table de-zoomcamp-449719:dlt_ipykernel_launcher_dataset.temp was not found in location US. The package is aborted and cannot be retried.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLoadClientJobFailed\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:607\u001b[0m, in \u001b[0;36mPipeline.load\u001b[0;34m(self, destination, dataset_name, credentials, workers, raise_on_failed_jobs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m signals\u001b[38;5;241m.\u001b[39mdelayed_signals():\n\u001b[0;32m--> 607\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m info: LoadInfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_step_info(load_step)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/common/runners/pool_runner.py:91\u001b[0m, in \u001b[0;36mrun_pool\u001b[0;34m(config, run_f)\u001b[0m\n\u001b[1;32m     90\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning pool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43m_run_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# for next run\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     signals\u001b[38;5;241m.\u001b[39mraise_if_signalled()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/common/runners/pool_runner.py:84\u001b[0m, in \u001b[0;36mrun_pool.<locals>._run_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(run_f, Runnable):\n\u001b[0;32m---> 84\u001b[0m     run_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/load/load.py:638\u001b[0m, in \u001b[0;36mLoad.run\u001b[0;34m(self, pool)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_info_start_load_id(load_id)\n\u001b[0;32m--> 638\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_single_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TRunMetrics(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_storage\u001b[38;5;241m.\u001b[39mlist_normalized_packages()))\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/load/load.py:597\u001b[0m, in \u001b[0;36mLoad.load_single_package\u001b[0;34m(self, load_id, schema)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pending_exception:\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m pending_exception\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mLoadClientJobFailed\u001b[0m: Job for temp.949a865c3a.jsonl failed terminally in load 1739940854.5797727 with message 404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/de-zoomcamp-449719/jobs?uploadType=resumable: Not found: Table de-zoomcamp-449719:dlt_ipykernel_launcher_dataset.temp was not found in location US. The package is aborted and cannot be retried.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m dlt\u001b[38;5;241m.\u001b[39mpipeline(destination\u001b[38;5;241m=\u001b[39mbigquery(credentials\u001b[38;5;241m=\u001b[39mgcp_credentials2))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:225\u001b[0m, in \u001b[0;36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[1;32m    223\u001b[0m         trace_step \u001b[38;5;241m=\u001b[39m start_trace_step(trace, cast(TPipelineStep, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 225\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:274\u001b[0m, in \u001b[0;36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[1;32m    270\u001b[0m         ConfigSectionContext(\n\u001b[1;32m    271\u001b[0m             pipeline_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, sections\u001b[38;5;241m=\u001b[39msections, merge_style\u001b[38;5;241m=\u001b[39mmerge_func\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    273\u001b[0m     ):\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:746\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, data, destination, staging, dataset_name, credentials, table_name, write_disposition, columns, primary_key, schema, loader_file_format, table_format, schema_contract, refresh)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract(\n\u001b[1;32m    735\u001b[0m         data,\n\u001b[1;32m    736\u001b[0m         table_name\u001b[38;5;241m=\u001b[39mtable_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m         refresh\u001b[38;5;241m=\u001b[39mrefresh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh,\n\u001b[1;32m    744\u001b[0m     )\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(loader_file_format\u001b[38;5;241m=\u001b[39mloader_file_format)\n\u001b[0;32m--> 746\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:225\u001b[0m, in \u001b[0;36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[1;32m    223\u001b[0m         trace_step \u001b[38;5;241m=\u001b[39m start_trace_step(trace, cast(TPipelineStep, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 225\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:165\u001b[0m, in \u001b[0;36mwith_state_sync.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m should_extract_state \u001b[38;5;241m=\u001b[39m may_extract_state \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrestore_from_destination\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_state(extract_state\u001b[38;5;241m=\u001b[39mshould_extract_state):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:274\u001b[0m, in \u001b[0;36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[1;32m    270\u001b[0m         ConfigSectionContext(\n\u001b[1;32m    271\u001b[0m             pipeline_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, sections\u001b[38;5;241m=\u001b[39msections, merge_style\u001b[38;5;241m=\u001b[39mmerge_func\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    273\u001b[0m     ):\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:614\u001b[0m, in \u001b[0;36mPipeline.load\u001b[0;34m(self, destination, dataset_name, credentials, workers, raise_on_failed_jobs)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m l_ex:\n\u001b[1;32m    613\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_step_info(load_step)\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_step\u001b[38;5;241m.\u001b[39mcurrent_load_id, l_ex, step_info\n\u001b[1;32m    616\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01ml_ex\u001b[39;00m\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m: Pipeline execution failed at stage load with exception:\n\n<class 'dlt.load.exceptions.LoadClientJobFailed'>\nJob for temp.949a865c3a.jsonl failed terminally in load 1739940854.5797727 with message 404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/de-zoomcamp-449719/jobs?uploadType=resumable: Not found: Table de-zoomcamp-449719:dlt_ipykernel_launcher_dataset.temp was not found in location US. The package is aborted and cannot be retried."
     ]
    }
   ],
   "source": [
    "pipeline = dlt.pipeline(destination=bigquery(credentials=gcp_credentials2))\n",
    "pipeline.run([{\"key1\": \"value1\"}], table_name=\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.oauth2.service_account.Credentials at 0x791acc1a64b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "service_account.Credentials.from_service_account_file(\"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/new-de-zoomcamp-449719-9c27773d9a31.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "###############################\n",
    "# NYCTaxiDataLoader Definition\n",
    "###############################\n",
    "class NYCTaxiDataLoader:\n",
    "    def __init__(self, gcp_credentials_path, bucket_name, file_format, base_url=\"https://d37ci6vzurychx.cloudfront.net/trip-data\"):\n",
    "        \"\"\"\n",
    "        Initialize the NYC Taxi Data Loader.\n",
    "        \n",
    "        Args:\n",
    "            gcp_credentials_path (str): Path to GCP service account JSON file.\n",
    "            bucket_name (str): Name of the GCS bucket.\n",
    "            file_format (str): File format to download ('parquet', 'csv', or 'excel').\n",
    "            base_url (str): Base URL for NYC taxi data.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.bucket_name = bucket_name\n",
    "        self.file_format = file_format.lower()\n",
    "        self.credentials = service_account.Credentials.from_service_account_file(gcp_credentials_path)\n",
    "        self.storage_client = storage.Client(credentials=self.credentials)\n",
    "        self.bucket = self.storage_client.bucket(bucket_name)\n",
    "        \n",
    "        # Set up logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def _generate_file_url(self, year, month):\n",
    "        \"\"\"\n",
    "        Generate the file URL for a given month/year.\n",
    "        For Excel files, the extension is assumed to be 'xlsx'.\n",
    "        \"\"\"\n",
    "        ext = self.file_format\n",
    "        if ext == \"excel\":\n",
    "            ext = \"xlsx\"\n",
    "        file_name = f\"yellow_tripdata_{year}-{month:02d}.{ext}\"\n",
    "        return f\"{self.base_url}/{file_name}\", file_name\n",
    "\n",
    "    def download_file(self, year, month):\n",
    "        \"\"\"\n",
    "        Download a single month's taxi data.\n",
    "        \"\"\"\n",
    "        url, file_name = self._generate_file_url(year, month)\n",
    "        local_path = f\"/tmp/{file_name}\"\n",
    "        try:\n",
    "            self.logger.info(f\"Downloading {file_name} from {url}\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            return local_path, file_name\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading {file_name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def upload_to_gcs(self, local_path, blob_name):\n",
    "        \"\"\"\n",
    "        Upload the local file to GCS.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            blob = self.bucket.blob(blob_name)\n",
    "            self.logger.info(f\"Uploading {blob_name} to GCS bucket {self.bucket_name}\")\n",
    "            blob.upload_from_filename(local_path)\n",
    "            os.remove(local_path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error uploading {blob_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def process_month(self, year, month):\n",
    "        \"\"\"\n",
    "        Process a single month (download then upload).\n",
    "        \"\"\"\n",
    "        result = self.download_file(year, month)\n",
    "        if result:\n",
    "            local_path, blob_name = result\n",
    "            return self.upload_to_gcs(local_path, blob_name)\n",
    "        return False\n",
    "\n",
    "    def process_months(self, year, start_month=1, end_month=7, max_workers=4):\n",
    "        \"\"\"\n",
    "        Process multiple months in parallel.\n",
    "        \"\"\"\n",
    "        successful_months = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_month = {\n",
    "                executor.submit(self.process_month, year, month): month\n",
    "                for month in range(start_month, end_month + 1)\n",
    "            }\n",
    "            for future in future_to_month:\n",
    "                month = future_to_month[future]\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        successful_months.append(month)\n",
    "                        self.logger.info(f\"Successfully processed month {month}\")\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Failed to process month {month}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing month {month}: {str(e)}\")\n",
    "        return successful_months\n",
    "\n",
    "#################################\n",
    "# Command-Line Argument Parsing\n",
    "#################################\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"NYCTaxiDataLoader + DLT Pipeline: Download NYC taxi data (in a single specified format) to GCS or run a DLT pipeline to load it to BigQuery.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--credentials',\n",
    "        required=True,\n",
    "        help='Path to GCP service account credentials JSON file'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--bucket',\n",
    "        required=True,\n",
    "        help='Name of the GCS bucket'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--year',\n",
    "        type=int,\n",
    "        default=datetime.now().year,\n",
    "        help='Year of data to download (default: current year)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--start-month',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        choices=range(1, 13),\n",
    "        help='Starting month (1-12)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--end-month',\n",
    "        type=int,\n",
    "        default=7,\n",
    "        choices=range(1, 13),\n",
    "        help='Ending month (1-12)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--workers',\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help='Number of parallel workers (default: 4)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--file-format',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        choices=['parquet', 'csv', 'excel'],\n",
    "        help='File format to process: parquet, csv, or excel'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--action',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        choices=['download', 'pipeline'],\n",
    "        help='Action to perform: \"download\" to load files into GCS, or \"pipeline\" to run the DLT pipeline to load from GCS to BigQuery'\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "#################################\n",
    "# DLT Pipeline Code\n",
    "#################################\n",
    "# Import DLT libraries and filesystem utilities\n",
    "import dlt\n",
    "from dlt.common.storages.fsspec_filesystem import FileItemDict\n",
    "from dlt.common.typing import TDataItems\n",
    "from dlt.sources.filesystem import filesystem, read_parquet, read_csv\n",
    "from typing import Iterator\n",
    "\n",
    "def get_bucket_url(bucket_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Construct the filesystem URL for GCS.\n",
    "    (For GCS, the URL scheme is typically \"gs://\")\n",
    "    \"\"\"\n",
    "    return f\"gs://{bucket_name}\"\n",
    "\n",
    "# Custom transformer to read Excel files using yield.\n",
    "@dlt.transformer(standalone=True)\n",
    "def read_excel(items: Iterator[FileItemDict], sheet_name: str) -> Iterator[TDataItems]:\n",
    "    import pandas as pd\n",
    "    for file_obj in items:\n",
    "        with file_obj.open() as f:\n",
    "            df = pd.read_excel(f, sheet_name=sheet_name)\n",
    "            for record in df.to_dict(orient=\"records\"):\n",
    "                yield record\n",
    "\n",
    "def run_dlt_pipeline(bucket_name: str, file_format: str,gcp_credentials_path: str,  sheet_name: str = \"Sheet1\"):\n",
    "    \"\"\"\n",
    "    Run the DLT pipeline that reads files from the GCS bucket and loads them to BigQuery.\n",
    "    Only files matching the specified file format are processed.\n",
    "    \"\"\"\n",
    "    BUCKET_URL = get_bucket_url(bucket_name)\n",
    "    # Choose the appropriate file resource based on the file_format.\n",
    "    if file_format.lower() == \"csv\":\n",
    "        # file_glob=\"**/*yellow_tripdata*.parquet\" to consider only yellow_tripdata\n",
    "        resource = filesystem(\n",
    "            bucket_url=BUCKET_URL,\n",
    "            file_glob=\"**/*.csv\"\n",
    "        ) | read_csv()\n",
    "    elif file_format.lower() == \"parquet\":\n",
    "        resource = filesystem(\n",
    "            bucket_url=BUCKET_URL,\n",
    "            file_glob=\"**/*.parquet\"\n",
    "        ) | read_parquet()\n",
    "    elif file_format.lower() == \"excel\":\n",
    "        resource = filesystem(\n",
    "            bucket_url=BUCKET_URL,\n",
    "            file_glob=\"**/*.xlsx\"\n",
    "        ) | read_excel(sheet_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "\n",
    "    with open(gcp_credentials_path, \"r\") as f:\n",
    "        creds2 = json.load(f)\n",
    "\n",
    "    gcp_credentials = GcpServiceAccountCredentials()\n",
    "    gcp_credentials.parse_native_representation(json.dumps(creds2))\n",
    "    # Define the DLT pipeline. In this example, we load into BigQuery.\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"nyc_taxi_pipeline\",\n",
    "        destination=bigquery(credentials=gcp_credentials),\n",
    "        dataset_name=\"nyc_taxi_data\"\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline loading the data into a table called \"nyc_taxi_table\".\n",
    "    load_info = pipeline.run(resource.with_name(\"nyc_taxi_table\"))\n",
    "    print(load_info)\n",
    "\n",
    "###############################\n",
    "# Main Function\n",
    "###############################\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # The user-specified file format controls both the download/upload and the DLT processing.\n",
    "    if args.action == \"download\":\n",
    "        loader = NYCTaxiDataLoader(args.credentials, args.bucket, args.file_format)\n",
    "        successful_months = loader.process_months(\n",
    "            year=args.year,\n",
    "            start_month=args.start_month,\n",
    "            end_month=args.end_month,\n",
    "            max_workers=args.workers\n",
    "        )\n",
    "        total_months = args.end_month - args.start_month + 1\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Successfully processed months: {successful_months}\")\n",
    "        print(f\"Total successful: {len(successful_months)}\")\n",
    "        print(f\"Total failed: {total_months - len(successful_months)}\")\n",
    "    elif args.action == \"pipeline\":\n",
    "        # For Excel files, you can adjust the sheet name as needed.\n",
    "        run_dlt_pipeline(args.bucket, args.file_format, args.credentials, sheet_name=\"Sheet1\")\n",
    "    else:\n",
    "        print(\"Invalid action specified. Use 'download' or 'pipeline'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python combined_script.py --credentials path/to/creds.json --bucket your-gcs-bucket --year 2021 --start-month 1 --end-month 3 --file-format csv --action download\n",
    "\n",
    "# python combined_script.py --credentials path/to/creds.json --bucket your-gcs-bucket --file-format csv --action pipeline\n",
    "\n",
    "#or use os.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = to set dlt bigquery env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using load_files_to_gcp_bigquery.py (this is slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python combined_script.py --credentials path/to/creds.json --bucket your-gcs-bucket --year 2021 --start-month 1 --end-month 3 --file-format csv --action download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "2025-02-19 07:15:23,529|[WARNING]|75452|126712164926080|dlt|pool_runner.py|run_pool:99|Exiting runner due to signal 2\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_gcp_bigquery.py\", line 279, in <module>\n",
      "    main()\n",
      "  File \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_gcp_bigquery.py\", line 274, in main\n",
      "    run_dlt_pipeline(args.bucket, args.file_format, args.credentials, sheet_name=\"Sheet1\")\n",
      "  File \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_gcp_bigquery.py\", line 248, in run_dlt_pipeline\n",
      "    load_info = pipeline.run(resource.with_name(\"nyc_taxi_table\"))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 225, in _wrap\n",
      "    step_info = f(self, *args, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 274, in _wrap\n",
      "    return f(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 745, in run\n",
      "    self.normalize(loader_file_format=loader_file_format)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 225, in _wrap\n",
      "    step_info = f(self, *args, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 179, in _wrap\n",
      "    rv = f(self, *args, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 274, in _wrap\n",
      "    return f(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 552, in normalize\n",
      "    runner.run_pool(normalize_step.config, normalize_step)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/common/runners/pool_runner.py\", line 91, in run_pool\n",
      "    while _run_func():\n",
      "          ^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/common/runners/pool_runner.py\", line 84, in _run_func\n",
      "    run_metrics = run_f.run(cast(TExecutor, pool))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/normalize/normalize.py\", line 280, in run\n",
      "    self.spool_schema_files(load_id, schema, schema_files)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/normalize/normalize.py\", line 234, in spool_schema_files\n",
      "    self.spool_files(load_id, schema.clone(update_normalizers=True), map_f, files)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/normalize/normalize.py\", line 166, in spool_files\n",
      "    schema_updates, writer_metrics = map_f(schema, load_id, files)\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/normalize/normalize.py\", line 147, in map_single\n",
      "    result = w_normalize_files(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/normalize/worker.py\", line 243, in w_normalize_files\n",
      "    partial_updates = normalizer(extracted_items_file, root_table_name)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/normalize/items_normalizers.py\", line 204, in __call__\n",
      "    partial_update = self._normalize_chunk(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/normalize/items_normalizers.py\", line 188, in _normalize_chunk\n",
      "    signals.raise_if_signalled()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/common/runtime/signals.py\", line 32, in raise_if_signalled\n",
      "    raise SignalReceivedException(_received_signal)\n",
      "dlt.common.exceptions.SignalReceivedException: Signal 2 received\n"
     ]
    }
   ],
   "source": [
    "!python /workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_gcp_bigquery.py --credentials \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/new-de-zoomcamp-449719-9c27773d9a31.json\" --bucket habeeb-babat-kestra --file-format parquet --action pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using load_files_to_bigquery_dlt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download only green taxi data for Jan-Mar 2023\n",
    "#python nyc_taxi_loader.py --action download --data-type green --year 2023 --start-month 1 --end-month 3 --file-format parquet --bucket my-taxi-bucket --credentials path/to/creds.json\n",
    "\n",
    "# Run pipeline for both yellow and green taxi data\n",
    "#python nyc_taxi_loader.py --action pipeline --data-type both --year 2023 --file-format parquet --bucket my-taxi-bucket --credentials path/to/creds.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 07:38:24 - root - INFO - Starting application with action=pipeline, data_type=both, format=parquet\n",
      "2025-02-19 07:38:24 - root - INFO - Running DLT pipeline for both taxi data in parquet format\n",
      "2025-02-19 07:38:24 - root - INFO - Filtering for year=2024, months=[1, 2, 3, 4, 5, 6, 7]\n",
      "2025-02-19 07:38:24 - dlt.pipeline - INFO - Starting DLT pipeline for both taxi data in parquet format from habeeb-babat-kestra\n",
      "2025-02-19 07:38:24 - dlt.pipeline - INFO - Filtering for data_type=both, year=2024, months=[1, 2, 3, 4, 5, 6, 7]\n",
      "2025-02-19 07:38:24 - dlt.pipeline - INFO - Setting up filesystem resource with glob pattern: *_tripdata_*.parquet\n",
      "2025-02-19 07:38:24 - dlt.pipeline - INFO - Using optimized Parquet reader\n",
      "2025-02-19 07:38:24 - dlt.pipeline - INFO - Initializing pipeline: nyc_taxi_both_parquet_pipeline\n",
      "2025-02-19 07:38:26 - root - INFO - FileItemDict content: {'file_name': 'yellow_tripdata_2024-01.parquet', 'relative_path': 'yellow_tripdata_2024-01.parquet', 'file_url': 'gs://habeeb-babat-kestra/yellow_tripdata_2024-01.parquet', 'mime_type': 'application/parquet', 'encoding': None, 'modification_date': DateTime(2025, 2, 10, 2, 31, 13, 418000, tzinfo=Timezone('UTC')), 'size_in_bytes': 49961641}\n",
      "2025-02-19 07:38:26,426|[ERROR]|98089|127091381150336|dlt.pipeline|load_files_to_bigquery_dlt.py|run_dlt_pipeline:400|Pipeline failed: Pipeline execution failed at stage extract when processing package 1739950704.8664978 with exception:\n",
      "\n",
      "<class 'dlt.extract.exceptions.ResourceExtractionError'>\n",
      "In processing pipe nyc_taxi_table: extraction of resource nyc_taxi_table in generator read_parquet_optimized caused an exception: 'FileItemDict' object has no attribute 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/extract/pipe_iterator.py\", line 277, in _get_source_item\n",
      "    pipe_item = next(gen)\n",
      "                ^^^^^^^^^\n",
      "StopIteration\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/extract/pipe_iterator.py\", line 277, in _get_source_item\n",
      "    pipe_item = next(gen)\n",
      "                ^^^^^^^^^\n",
      "  File \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_bigquery_dlt.py\", line 176, in read_parquet_optimized\n",
      "    file_name = file_obj.name.split('/')[-1]\n",
      "                ^^^^^^^^^^^^^\n",
      "AttributeError: 'FileItemDict' object has no attribute 'name'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 470, in extract\n",
      "    self._extract_source(\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 1238, in _extract_source\n",
      "    load_id = extract.extract(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/extract/extract.py\", line 435, in extract\n",
      "    self._extract_single_source(\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/extract/extract.py\", line 358, in _extract_single_source\n",
      "    for pipe_item in pipes:\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/extract/pipe_iterator.py\", line 162, in __next__\n",
      "    pipe_item = self._get_source_item()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/extract/pipe_iterator.py\", line 303, in _get_source_item\n",
      "    return self._get_source_item()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/extract/pipe_iterator.py\", line 307, in _get_source_item\n",
      "    raise ResourceExtractionError(pipe.name, gen, str(ex), \"generator\") from ex\n",
      "dlt.extract.exceptions.ResourceExtractionError: In processing pipe nyc_taxi_table: extraction of resource nyc_taxi_table in generator read_parquet_optimized caused an exception: 'FileItemDict' object has no attribute 'name'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_bigquery_dlt.py\", line 521, in <module>\n",
      "    main()\n",
      "  File \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_bigquery_dlt.py\", line 508, in main\n",
      "    run_dlt_pipeline(\n",
      "  File \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_bigquery_dlt.py\", line 395, in run_dlt_pipeline\n",
      "    load_info = pipeline.run(resource.with_name(table_name))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 225, in _wrap\n",
      "    step_info = f(self, *args, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 274, in _wrap\n",
      "    return f(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 734, in run\n",
      "    self.extract(\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 225, in _wrap\n",
      "    step_info = f(self, *args, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 179, in _wrap\n",
      "    rv = f(self, *args, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 165, in _wrap\n",
      "    return f(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 274, in _wrap\n",
      "    return f(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/dlt/pipeline/pipeline.py\", line 491, in extract\n",
      "    raise PipelineStepFailed(\n",
      "dlt.pipeline.exceptions.PipelineStepFailed: Pipeline execution failed at stage extract when processing package 1739950704.8664978 with exception:\n",
      "\n",
      "<class 'dlt.extract.exceptions.ResourceExtractionError'>\n",
      "In processing pipe nyc_taxi_table: extraction of resource nyc_taxi_table in generator read_parquet_optimized caused an exception: 'FileItemDict' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# Download only green taxi data for Jan-Mar 2023\n",
    "#python nyc_taxi_loader.py --action download --data-type green --year 2023 --start-month 1 --end-month 3 --file-format parquet --bucket my-taxi-bucket --credentials path/to/creds.json\n",
    "\n",
    "# Run pipeline for both yellow and green taxi data\n",
    "!python /workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_bigquery_dlt.py --action pipeline --data-type both --year 2024 --file-format parquet --bucket habeeb-babat-kestra --credentials \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/new-de-zoomcamp-449719-9c27773d9a31.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 15:34:43 - root - INFO - Starting application with action=pipeline, data_type=both, format=parquet\n",
      "2025-02-19 15:34:43 - root - INFO - Running DLT pipeline for both taxi data in parquet format\n",
      "2025-02-19 15:34:43 - root - INFO - Filtering for year=2024, months=[1, 2, 3, 4, 5, 6]\n",
      "2025-02-19 15:34:43 - dlt.pipeline - INFO - Starting DLT pipeline for both taxi data in parquet format from habeeb-babat-kestra\n",
      "2025-02-19 15:34:43 - dlt.pipeline - INFO - Filtering for data_type=both, year=2024, months=[1, 2, 3, 4, 5, 6]\n",
      "2025-02-19 15:34:43 - dlt.pipeline - INFO - Setting up filesystem resource with glob pattern: *_tripdata_*.parquet\n",
      "2025-02-19 15:34:43 - dlt.pipeline - INFO - Using optimized Parquet reader\n",
      "2025-02-19 15:34:43 - dlt.pipeline - INFO - Initializing pipeline: nyc_taxi_both_parquet_pipeline\n",
      "2025-02-19 15:34:46 - root - INFO - FileItemDict content: {'file_name': 'yellow_tripdata_2024-01.parquet', 'relative_path': 'yellow_tripdata_2024-01.parquet', 'file_url': 'gs://habeeb-babat-kestra/yellow_tripdata_2024-01.parquet', 'mime_type': 'application/parquet', 'encoding': None, 'modification_date': DateTime(2025, 2, 10, 2, 31, 13, 418000, tzinfo=Timezone('UTC')), 'size_in_bytes': 49961641}\n",
      "2025-02-19 15:36:26 - root - INFO - FileItemDict content: {'file_name': 'yellow_tripdata_2024-02.parquet', 'relative_path': 'yellow_tripdata_2024-02.parquet', 'file_url': 'gs://habeeb-babat-kestra/yellow_tripdata_2024-02.parquet', 'mime_type': 'application/parquet', 'encoding': None, 'modification_date': DateTime(2025, 2, 10, 2, 31, 18, 176000, tzinfo=Timezone('UTC')), 'size_in_bytes': 50349284}\n",
      "2025-02-19 15:38:08 - root - INFO - FileItemDict content: {'file_name': 'yellow_tripdata_2024-03.parquet', 'relative_path': 'yellow_tripdata_2024-03.parquet', 'file_url': 'gs://habeeb-babat-kestra/yellow_tripdata_2024-03.parquet', 'mime_type': 'application/parquet', 'encoding': None, 'modification_date': DateTime(2025, 2, 10, 2, 31, 23, 79000, tzinfo=Timezone('UTC')), 'size_in_bytes': 60078280}\n",
      "2025-02-19 15:40:08 - root - INFO - FileItemDict content: {'file_name': 'yellow_tripdata_2024-04.parquet', 'relative_path': 'yellow_tripdata_2024-04.parquet', 'file_url': 'gs://habeeb-babat-kestra/yellow_tripdata_2024-04.parquet', 'mime_type': 'application/parquet', 'encoding': None, 'modification_date': DateTime(2025, 2, 10, 2, 31, 29, 225000, tzinfo=Timezone('UTC')), 'size_in_bytes': 59133625}\n",
      "2025-02-19 15:42:03 - root - INFO - FileItemDict content: {'file_name': 'yellow_tripdata_2024-05.parquet', 'relative_path': 'yellow_tripdata_2024-05.parquet', 'file_url': 'gs://habeeb-babat-kestra/yellow_tripdata_2024-05.parquet', 'mime_type': 'application/parquet', 'encoding': None, 'modification_date': DateTime(2025, 2, 10, 2, 31, 34, 273000, tzinfo=Timezone('UTC')), 'size_in_bytes': 62553128}\n",
      "2025-02-19 15:44:07 - root - INFO - FileItemDict content: {'file_name': 'yellow_tripdata_2024-06.parquet', 'relative_path': 'yellow_tripdata_2024-06.parquet', 'file_url': 'gs://habeeb-babat-kestra/yellow_tripdata_2024-06.parquet', 'mime_type': 'application/parquet', 'encoding': None, 'modification_date': DateTime(2025, 2, 10, 2, 31, 40, 335000, tzinfo=Timezone('UTC')), 'size_in_bytes': 59859922}\n"
     ]
    }
   ],
   "source": [
    "!python /workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/load_files_to_gcp_bigquery_dlt.py --action pipeline --data-type both --year 2024 --file-format parquet --bucket habeeb-babat-kestra --credentials \"/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/new-de-zoomcamp-449719-9c27773d9a31.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
