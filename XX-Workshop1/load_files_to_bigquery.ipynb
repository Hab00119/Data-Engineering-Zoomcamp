{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('/workspaces/Data-Engineering-Zoomcamp/XX-Workshop1/new-de-zoomcamp-449719-9c27773d9a31.json', 'r') as f:\n",
    "    os.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 05:16:47,480 - INFO - Downloading yellow_tripdata_2024-01.parquet\n",
      "2025-02-18 05:16:48,536 - INFO - Successfully processed yellow_tripdata_2024-01.parquet, found 2964624 records\n",
      "2025-02-18 05:17:14,269 - INFO - Downloading yellow_tripdata_2024-02.parquet\n",
      "2025-02-18 05:17:17,306 - INFO - Successfully processed yellow_tripdata_2024-02.parquet, found 3007526 records\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import dlt\n",
    "from dlt.sources import DltResource\n",
    "from typing import List, Dict, Iterator, Any, Tuple, Optional\n",
    "\n",
    "@dlt.source\n",
    "def nyc_taxi_data(\n",
    "    year: int = datetime.now().year,\n",
    "    start_month: int = 1,\n",
    "    end_month: int = 7,\n",
    "    max_workers: int = 1,\n",
    "    base_url: str = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    ") -> List[DltResource]:\n",
    "    \"\"\"\n",
    "    A dlt source for NYC Taxi trip data.\n",
    "\n",
    "    Args:\n",
    "        year: Year of the data to download (default: current year)\n",
    "        start_month: Starting month (1-12, default: 1)\n",
    "        end_month: Ending month (1-12, default: 7)\n",
    "        max_workers: Maximum number of parallel workers (default: 4)\n",
    "        base_url: Base URL for NYC taxi data (default: \"https://d37ci6vzurychx.cloudfront.net/trip-data\")\n",
    "\n",
    "    Returns:\n",
    "        List of dlt resources for NYC taxi data\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    @dlt.resource(write_disposition=\"merge\", primary_key=[\"vendorID\", \"tpep_pickup_datetime\"])\n",
    "    def yellow_taxi_trips() -> Iterator[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Resource that yields NYC yellow taxi trip data for a specified time range.\n",
    "        Data is downloaded and yielded in chunks for memory efficiency.\n",
    "        \"\"\"\n",
    "        def download_month_data(year: int, month: int) -> Optional[List[Dict[str, Any]]]:\n",
    "            file_name = f\"yellow_tripdata_{year}-{month:02d}.parquet\"\n",
    "            url = f\"{base_url}/{file_name}\"\n",
    "            \n",
    "            try:\n",
    "                logger.info(f\"Downloading {file_name}\")\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Save to a temporary file\n",
    "                temp_path = f\"/tmp/{file_name}\"\n",
    "                with open(temp_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1000):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                # Read parquet file into a pandas DataFrame and convert to dict\n",
    "                import pandas as pd\n",
    "                df = pd.read_parquet(temp_path)\n",
    "                \n",
    "                # Clean up temporary file\n",
    "                os.remove(temp_path)\n",
    "                \n",
    "                # Add metadata columns for tracking\n",
    "                df['source_file'] = file_name\n",
    "                df['ingestion_timestamp'] = datetime.now().isoformat()\n",
    "                \n",
    "                logger.info(f\"Successfully processed {file_name}, found {len(df)} records\")\n",
    "                return df.to_dict('records')\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error downloading {file_name}: {str(e)}\")\n",
    "                return None\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(download_month_data, year, month): (year, month)\n",
    "                for month in range(start_month, end_month + 1)\n",
    "            }\n",
    "            \n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    for record in result:\n",
    "                        yield record\n",
    "    \n",
    "    return [yellow_taxi_trips]\n",
    "\n",
    "def run_nyc_taxi_pipeline():\n",
    "    \"\"\"\n",
    "    Main function to run the NYC taxi pipeline to BigQuery\n",
    "    \"\"\"\n",
    "    # Configure the pipeline to load data to BigQuery\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name='nyc_taxi_data',\n",
    "        destination='bigquery',\n",
    "        dataset_name='nyc_taxi',\n",
    "        dev_mode=True\n",
    "    )\n",
    "    \n",
    "    # Load the data\n",
    "    load_info = pipeline.run(\n",
    "        nyc_taxi_data(\n",
    "            year=2024,  # can be parameterized\n",
    "            start_month=1,  # can be parameterized\n",
    "            end_month=2,   # can be parameterized\n",
    "            max_workers=1  # can be parameterized\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Load info: {load_info}\")\n",
    "    \n",
    "    # Return info for verification\n",
    "    return load_info\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_nyc_taxi_pipeline()\n",
    "\n",
    "# To customize pipeline for BigQuery, you can use the following:\n",
    "\"\"\"\n",
    "import dlt\n",
    "\n",
    "# Create credentials from service account key file\n",
    "credentials = dlt.secrets.service_account_key_file('/path/to/credentials.json')\n",
    "\n",
    "# Initialize pipeline with BigQuery-specific configuration\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='nyc_taxi_data',\n",
    "    destination='bigquery',\n",
    "    dataset_name='nyc_taxi',\n",
    "    credentials=credentials,\n",
    "    full_refresh=False, # set to True to replace all data\n",
    "    progress=True\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline.run(nyc_taxi_data())\n",
    "\"\"\"\n",
    "\n",
    "# You can also run from command line with:\n",
    "# python nyc_taxi_dlt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
